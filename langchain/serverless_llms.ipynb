{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os \n",
        "\n",
        "load_dotenv()\n",
        "MISTRAL_ENDPOINT = os.getenv(\"AZ_MISTRAL_ENDPOINT\")\n",
        "MISTRAL_API_KEY = os.getenv(\"AZ_MISTRAL_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Not all LLMs have the `.with_structured_output` implemented. See LangChain docs for all supported ones. \n",
        "\n",
        "https://python.langchain.com/v0.1/docs/modules/model_io/chat/structured_output/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Mistral Small"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "reference:\n",
        "\n",
        "https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/mistral/langchain.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "editable": true,
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "source": [
        "### a. Basic Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1716519159911
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain_mistralai.chat_models import ChatMistralAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1716519174087
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=(\n",
        "                \"You are a helpful assistant that is specialized in sport\"\n",
        "            )\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "# memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1716521273523
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# chat_llm_chain = LLMChain(\n",
        "#     llm=chat_model,\n",
        "#     prompt=prompt,\n",
        "#     memory=memory,\n",
        "#     verbose=True,\n",
        "# )\n",
        "\n",
        "llm = ChatMistralAI(\n",
        "    endpoint=MISTRAL_ENDPOINT,\n",
        "    mistral_api_key=MISTRAL_API_KEY,\n",
        ")\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1716521280921
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "ConnectError",
          "evalue": "[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_transports\\default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:99\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:76\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_sync\\connection.py:154\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_tls\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m--> 154\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_tls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_backends\\sync.py:152\u001b[0m, in \u001b[0;36mSyncStream.start_tls\u001b[1;34m(self, ssl_context, server_hostname, timeout)\u001b[0m\n\u001b[0;32m    148\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    149\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[0;32m    151\u001b[0m }\n\u001b[1;32m--> 152\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpcore\\_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[1;34m(map)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[1;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mConnectError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_input\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgive me a fun fact about Neymar Jr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2495\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2494\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2495\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2496\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2497\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:170\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    167\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    169\u001b[0m         ChatGeneration,\n\u001b[1;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    180\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:599\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    593\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    598\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:456\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    455\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    457\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    458\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    460\u001b[0m ]\n\u001b[0;32m    461\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:446\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 446\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m         )\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:671\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 671\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:457\u001b[0m, in \u001b[0;36mChatMistralAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    456\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m--> 457\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:380\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m         _raise_on_error(response)\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m--> 380\u001b[0m rtn \u001b[38;5;241m=\u001b[39m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rtn\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:376\u001b[0m, in \u001b[0;36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iter_sse()\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     _raise_on_error(response)\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_client.py:1145\u001b[0m, in \u001b[0;36mClient.post\u001b[1;34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1126\u001b[0m     url: URLTypes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     extensions: RequestExtensions \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1139\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m \u001b[38;5;124;03m    Send a `POST` request.\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m \n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_client.py:827\u001b[0m, in \u001b[0;36mClient.request\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    812\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m    814\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    815\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    816\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    825\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    826\u001b[0m )\n\u001b[1;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    910\u001b[0m )\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_transports\\default.py:232\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(request\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[1;32m--> 232\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    156\u001b[0m     value \u001b[38;5;241m=\u001b[39m typ()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
            "File \u001b[1;32mc:\\Users\\JOSHUALE\\Anaconda3\\envs\\langchain-demo-env\\Lib\\site-packages\\httpx\\_transports\\default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     85\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
            "\u001b[1;31mConnectError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1006)"
          ]
        }
      ],
      "source": [
        "chain.invoke({\"user_input\": \"give me a fun fact about Neymar Jr\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### b. Extract Structured Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1716541274283
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Optional, List\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1716541276283
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "body = \"\"\"\n",
        "Officials involved in the clear up of the bridge collapse at Baltimore Port in the US have said the vessel that remains lodged among debris will be removed by 10 May.\n",
        "While some ships have been able to navigate in and out of the port through a limited access channel opened up by the coastguard in the middle of the collapsed bridge, the Dali containership has remained in the place where it collided with the Francis Scott Key Bridge since the incident took place at the major port in Maryland on 26 March.\n",
        "Ahead of the planned removal of the ship, a 35ft deep Fort McHenry Limited Access Channel that had been open for four days and allowed the first container ship to return to the port closed on 29 April, though the three other temporary channels, which are 20, 14 and 11ft deep, will remain open.\n",
        "Maryland Governor Wes Moore highlighted some of the difficulties that have been faced by the team attempting to clear the bridge debris and Dali vessel.\n",
        "\"That work is remarkably complicated, we're talking about a massive piece of steel,\" he said.\n",
        "\"On one end the steel is leaning against a vessel that is the size of the Eiffel Tower and, on the other end, it is leaning against the bottom of the riverbed, so this work is dangerous.\"\n",
        "Additionally, the clearance operation is also being run hand in hand with the continuing recovery operations for the two roadworkers still missing after falling with the bridge, with only four bodies recovered from the wreckage so far.\n",
        "While the Maryland Government and Port of Baltimore provided further details about the removal of the vessel, the authorities would not be drawn on how much longer the cleanup and recovery effort could take.\n",
        "However, the authorities have set the end of May as a target date for the reopening of the Port of Baltimore's permanent 50ft deep and 700ft wide channel, with an initial 45ft channel expected to open when the ship is removed around 10 May.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1716541278252
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "class NewsInfo(BaseModel):\n",
        "    \"\"\"Information extracted from the text.\"\"\"\n",
        "    title: str = Field(\n",
        "        description=\"One sentence summary of the article of maximum 200 characters, prefereably with the event, location and time information.\"\n",
        "    )\n",
        "    summary: str = Field(\n",
        "        description=\"A short summary of the text, maximum 200 words\"\n",
        "    )\n",
        "    impact: str = Field( \n",
        "        description=\"Answer only Yes or No to this question: does this event negatively impact a supply chain network (the movement of people and goods)? Answer this by following the following reasoning steps: \\\n",
        "            If the event can directly impact a supply chain network in a negative way, such as causing facility damage or traffic stopage, etc., then Yes. \\\n",
        "            Else if it can potentially disrupt the normal operations a supply chain network, such as social-political disruptions, extreme weathers, or other disruptions, etc., then asnwer Yes. \\\n",
        "            If not or uncertain, such as general knowledge, good news, individual personnel events, project annoucement etc., answer No\",\n",
        "        enum=[\"Yes\", \"No\"]\n",
        "    )\n",
        "    reasoning: str = Field( \n",
        "        description=\"The reasoning behind your impact assessment based on the impact reasoning step above. Explain why you think the event will (Yes) or will not (No) impact the supply chain network.\"\n",
        "    )\n",
        "    vessel_name: Optional[list[str]] = Field(\n",
        "        default=[\"\"], \n",
        "        description=\"The names of the marine vessels or container ships mentioned in the text, if any.\"\n",
        "    )\n",
        "\n",
        "news_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm, specialized in news analysis.\"\n",
        "            \"Only extract relevant information from the text. \"\n",
        "            \"If you do not know the value of an attribute asked to extract, \"\n",
        "            \"return 'Uncertain' for the attribute's value.\",\n",
        "        ),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1716519552327
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Bridge Collapse at Baltimore Port: Vessel Removal Planned for 10 May, Cleanup Effort Ongoing',\n",
              " 'summary': 'The Dali containership, which has been lodged among debris since the collapse of the Francis Scott Key Bridge on 26 March, is set to be removed by 10 May. The clearance operation is being run alongside recovery efforts for the two missing roadworkers. The Port of Baltimore aims to reopen its permanent channel by the end of May. ',\n",
              " 'impact': 'Yes',\n",
              " 'reasoning': 'The bridge collapse and the lodged vessel are directly impacting the supply chain network by causing facility damage and disrupting the normal operations, such as limiting access to the port and delaying the movement of goods.',\n",
              " 'vessel_name': ['Dali']}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm = chat_model\n",
        "sample_text = body\n",
        "news_runnable = news_prompt | llm.with_structured_output(schema=NewsInfo)\n",
        "body_structured = news_runnable.invoke({\"user_input\": sample_text})\n",
        "body_structured.dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c. Implement alternative to the with_structured_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import PydanticOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"description\": \"Information extracted from the text.\", \"properties\": {\"title\": {\"title\": \"Title\", \"description\": \"One sentence summary of the article of maximum 200 characters, prefereably with the event, location and time information.\", \"type\": \"string\"}, \"summary\": {\"title\": \"Summary\", \"description\": \"A short summary of the text, maximum 200 words\", \"type\": \"string\"}, \"impact\": {\"title\": \"Impact\", \"description\": \"Answer only Yes or No to this question: does this event negatively impact a supply chain network (the movement of people and goods)? Answer this by following the following reasoning steps:             If the event can directly impact a supply chain network in a negative way, such as causing facility damage or traffic stopage, etc., then Yes.             Else if it can potentially disrupt the normal operations a supply chain network, such as social-political disruptions, extreme weathers, or other disruptions, etc., then asnwer Yes.             If not or uncertain, such as general knowledge, good news, individual personnel events, project annoucement etc., answer No\", \"enum\": [\"Yes\", \"No\"], \"type\": \"string\"}, \"reasoning\": {\"title\": \"Reasoning\", \"description\": \"The reasoning behind your impact assessment based on the impact reasoning step above. Explain why you think the event will (Yes) or will not (No) impact the supply chain network.\", \"type\": \"string\"}, \"vessel_name\": {\"title\": \"Vessel Name\", \"description\": \"The names of the marine vessels or container ships mentioned in the text, if any.\", \"default\": [\"\"], \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"title\", \"summary\", \"impact\", \"reasoning\"]}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "pydantic_parser = PydanticOutputParser(pydantic_object=NewsInfo)\n",
        "format_instructions = pydantic_parser.get_format_instructions()\n",
        "print(format_instructions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEWS_PARSING_PROMPT = \"\"\"\n",
        "You are an expert extraction algorithm, specialized in news analysis.\n",
        "Your goal is to understand and parse out the news article content based on the user's instructions of the output schema.\n",
        "Only ouput the result into the schema without generating any other information outside the schema.\n",
        "{format_instructions}\n",
        "\n",
        "news article content:\n",
        "{news_content}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    template=NEWS_PARSING_PROMPT,\n",
        "    partial_variables = {\n",
        "        \"format_instructions\": format_instructions # passing in the formatting instructions created earlier in place of \"format_instructions\" placeholder\n",
        "    }\n",
        ")\n",
        "\n",
        "full_chain = {\"news_content\": lambda x: x[\"news_content\"]} | prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = full_chain.invoke({\"news_content\": sample_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Dali containership to be removed from Baltimore Port by 10 May, bridge cleanup continues',\n",
              " 'summary': 'The Dali containership, which has been stuck at Baltimore Port since 26 March, is set to be removed by 10 May. The bridge cleanup is ongoing, with some ships able to navigate through limited access channels. The Maryland Government and Port of Baltimore have set the end of May as a target date for the reopening of the permanent port channel.',\n",
              " 'impact': 'Yes',\n",
              " 'reasoning': 'The incident is directly impacting the supply chain network by causing a blockage in the Baltimore Port. The stuck vessel and the bridge debris are preventing some ships from navigating through the port, disrupting the normal operations of the supply chain network.',\n",
              " 'vessel_name': ['Dali']}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. LLaMA-2-70B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Reference:\n",
        "\n",
        "https://github.com/Azure/azureml-examples/blob/main/sdk/python/foundation-models/llama2/langchain.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1716542719961
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        ")\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain_community.chat_models.azureml_endpoint import (\n",
        "    AzureMLChatOnlineEndpoint,\n",
        "    AzureMLEndpointApiType,\n",
        "    LlamaChatContentFormatter,\n",
        ")\n",
        "\n",
        "LLAMA2_70B_ENDPOINT = os.getenv(\"LLAMA2_70B_ENDPOINT\")\n",
        "LLAMA2_70B_API_KEY = os.getenv(\"LLAMA2_70B_API_KEY\")\n",
        "ENDPOINT_URL = LLAMA2_70B_ENDPOINT + \"/v1/chat/completions\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### a. Testing the basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1716543180442
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/langchain_community/chat_models/azureml_endpoint.py:180: UserWarning: `LlamaChatContentFormatter` will be deprecated in the future. \n",
            "                Please use `CustomOpenAIChatContentFormatter` instead.  \n",
            "            \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# the original endpoint URI is \"https://Llama-2-70b-chat-dev-serverless.eastus2.inference.ai.azure.com\"\n",
        "# just take the \n",
        "\n",
        "chat_model = AzureMLChatOnlineEndpoint(\n",
        "    # endpoint_url=\"https://Llama-2-70b-chat-dev-serverless.eastus2.inference.ai.azure.com/v1/chat/completions\",\n",
        "    endpoint_url = ENDPOINT_URL,\n",
        "    endpoint_api_type=AzureMLEndpointApiType.serverless,\n",
        "    endpoint_api_key=LLAMA2_70B_API_KEY,\n",
        "    content_formatter=LlamaChatContentFormatter(),\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are a chatbot having a conversation with a human. You love making references to animals on your answers.\"\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{human_input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "chat_llm_chain = LLMChain(\n",
        "    llm=chat_model,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1716541173564
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot having a conversation with a human. You love making references to animals on your answers.\n",
            "Human: Hi there my friend\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Woof woof! Hello there, my human friend! It's a purrfect day to be having a chat with you. I'm feeling like a happy-go-lucky puppy today, and I hope you're feeling just as fabulous. What's on your mind? Do you have any questions for me, or shall we just have a friendly chat like a couple of old birds? \""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_llm_chain.predict(human_input=\"Hi there my friend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### b. Extract Structured Outputs - not yet supported (the model not trained on tool calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1716543206429
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m chat_model\n\u001b[1;32m      2\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m body\n\u001b[0;32m----> 3\u001b[0m news_runnable \u001b[38;5;241m=\u001b[39m news_prompt \u001b[38;5;241m|\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_structured_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNewsInfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m body_structured \u001b[38;5;241m=\u001b[39m news_runnable\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_input\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_text})\n\u001b[1;32m      5\u001b[0m body_structured\u001b[38;5;241m.\u001b[39mdict()\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/langchain_core/language_models/base.py:210\u001b[0m, in \u001b[0;36mBaseLanguageModel.with_structured_output\u001b[0;34m(self, schema, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Not implemented on this class.\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Implement this on child class if there is a way of steering the model to\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# generate responses that match a given schema.\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "llm = chat_model\n",
        "sample_text = body\n",
        "news_runnable = news_prompt | llm.with_structured_output(schema=NewsInfo)\n",
        "body_structured = news_runnable.invoke({\"user_input\": sample_text})\n",
        "body_structured.dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1716545056815
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/langchain_community/chat_models/azureml_endpoint.py:180: UserWarning: `LlamaChatContentFormatter` will be deprecated in the future. \n",
            "                Please use `CustomOpenAIChatContentFormatter` instead.  \n",
            "            \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# as the .with_structured_output is not implemented in langchain, we can try to implement it inside the prompt\n",
        "news_prompt_structured = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert extraction algorithm, specialized in news analysis.\"\n",
        "            \"Only extract relevant information from the text. \"\n",
        "            \"return a json with the following structure, do not generate anything else besides the JSON. Please close the JSON brackets properly\"\n",
        "            \"\"\"\n",
        "            \n",
        "                \"title\":\"[compulsory] One sentence summary of the article of maximum 200 characters, prefereably with the event, location and time information.\",\\\n",
        "                'summary':'[compulsory] A short summary of the text, maximum 200 words',\\\n",
        "                'impact':'[compulsory] Answer only Yes or No to this question: does this event negatively impact a supply chain network (the movement of people and goods)? Answer this by following the following reasoning steps: \\\n",
        "                                        If the event can directly impact a supply chain network in a negative way, such as causing facility damage or traffic stopage, etc., then Yes. \\\n",
        "                                        Else if it can potentially disrupt the normal operations a supply chain network, such as social-political disruptions, extreme weathers, or other disruptions, etc., then asnwer Yes. \\\n",
        "                                        If not or uncertain, such as general knowledge, good news, individual personnel events, project annoucement etc., answer No',\\\n",
        "                'impact':'[compulsory] The reasoning behind your impact assessment based on the impact reasoning step above. Explain why you think the event will (Yes) or will not (No) impact the supply chain network.',\\\n",
        "                'vessel_name':'[optioal] The names of the marine vessels or container ships mentioned in the text, if any.'\n",
        "            \n",
        "            \"\"\"\n",
        "        ),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "ENDPOINT_URL = LLAMA2_70B_ENDPOINT + \"/v1/chat/completions\"\n",
        "chat_model = AzureMLChatOnlineEndpoint(\n",
        "    endpoint_url = ENDPOINT_URL,\n",
        "    endpoint_api_type=AzureMLEndpointApiType.serverless,\n",
        "    endpoint_api_key=LLAMA2_70B_API_KEY,\n",
        "    content_formatter=LlamaChatContentFormatter(),\n",
        ")\n",
        "llm = chat_model\n",
        "sample_text = body\n",
        "news_runnable = news_prompt_structured | llm\n",
        "body_structured = news_runnable.invoke({\"user_input\": sample_text})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1716545057328
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n\"title\": \"Baltimore Port to Remove Vessel Stuck in Collapsed Bridge Debris by May 10\",\\n\"summary\": \"The Dali containership, which has been stuck in the debris of the collapsed Francis Scott Key Bridge since March 26, will be removed by May 10, according to officials involved in the clear up effort. The removal will allow the Port of Baltimore to reopen its permanent 50ft deep and 700ft wide channel, with an initial 45ft channel expected to open when the ship is removed.\",\\n\"impact\": \"Yes\",\\n\"impact_reasoning\": \"The event directly impacts the supply chain network by blocking the passage of ships through the Port of Baltimore, disrupting the normal operations of the port and potentially causing delays or rerouting of shipments. The removal of the vessel will allow the port to reopen its permanent channel, restoring normal operations and mitigating the impact on the supply chain.\",\\n\"vessel_name\": \"Dali\"\\n}'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "body_structured.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### c. Using alternative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/langchain-demo-env/lib/python3.11/site-packages/langchain_community/chat_models/azureml_endpoint.py:180: UserWarning: `LlamaChatContentFormatter` will be deprecated in the future. \n",
            "                Please use `CustomOpenAIChatContentFormatter` instead.  \n",
            "            \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "chat_model = AzureMLChatOnlineEndpoint(\n",
        "    # endpoint_url=\"https://Llama-2-70b-chat-dev-serverless.eastus2.inference.ai.azure.com/v1/chat/completions\",\n",
        "    endpoint_url = ENDPOINT_URL,\n",
        "    endpoint_api_type=AzureMLEndpointApiType.serverless,\n",
        "    endpoint_api_key=LLAMA2_70B_API_KEY,\n",
        "    content_formatter=LlamaChatContentFormatter(),\n",
        ")\n",
        "llm = chat_model\n",
        "full_chain = {\"news_content\": lambda x: x[\"news_content\"]} | prompt | llm\n",
        "result = full_chain.invoke({\"news_content\": sample_text}) # about 25s to run (vs 3s in Mistral small)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Here is the extracted information in the format of a JSON instance conforming to the provided schema:\\n\\n{\\n\"title\": \"Vessel Removal to Take Place at Baltimore Port by May 10\",\\n\"summary\": \"The Dali containership, which collided with the Francis Scott Key Bridge and has been lodged in the debris since March 26, is expected to be removed by May 10, according to officials involved in the clear-up efforts at Baltimore Port in the US. The removal is part of a larger operation that includes the recovery of two missing roadworkers and the reopening of the port\\'s permanent channel by the end of May.\",\\n\"impact\": \"Yes\",\\n\"reasoning\": \"The event directly impacts the supply chain network by disrupting the normal operations of the Port of Baltimore, which is a major transportation hub for goods and people. The collapse of the bridge has created a challenge in the removal of the vessel, and the authorities have set a target date of May 10 for its removal. This indicates that the event has the potential to cause significant disruption to the supply chain network, and therefore, the impact assessment is Yes.\",\\n\"vessel_name\": [\"Dali\"]\\n}\\n\\nNote: The \"vessel_name\" field contains an array of strings, as per the schema, since there is only one vessel name mentioned in the text.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# it can be seen that the model still output unnecessary information before the JSON output\n",
        "result.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can construct another prompt to filter out the unnecessary information, retaining only the JSON output\n",
        "POST_PROCESSING_PROMPT = \"\"\"\n",
        "Your goal is to pick up the JSON information from the input and output only the JSON information. \n",
        "Output only the JSON information and nothing else. Do not generate any other information outside the JSON.\n",
        "Correct the JSON format if necessary, such as missing brackets, but do not change the content of the JSON.\n",
        "Input content:\n",
        "{input_content}\n",
        "\"\"\"\n",
        "\n",
        "post_processing_prompt = ChatPromptTemplate.from_template(\n",
        "    template=POST_PROCESSING_PROMPT,\n",
        ")\n",
        "\n",
        "post_processing_chain = {\"input_content\": lambda x: x[\"input_content\"]} | post_processing_prompt | llm\n",
        "post_processing_result = post_processing_chain.invoke({\"input_content\": result.content})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n\"title\": \"Vessel Removal to Take Place at Baltimore Port by May 10\",\\n\"summary\": \"The Dali containership, which collided with the Francis Scott Key Bridge and has been lodged in the debris since March 26, is expected to be removed by May 10, according to officials involved in the clear-up efforts at Baltimore Port in the US. The removal is part of a larger operation that includes the recovery of two missing roadworkers and the reopening of the port\\'s permanent channel by the end of May.\",\\n\"impact\": \"Yes\",\\n\"reasoning\": \"The event directly impacts the supply chain network by disrupting the normal operations of the Port of Baltimore, which is a major transportation hub for goods and people. The collapse of the bridge has created a challenge in the removal of the vessel, and the authorities have set a target date of May 10 for its removal. This indicates that the event has the potential to cause significant disruption to the supply chain network, and therefore, the impact assessment is Yes.\",\\n\"vessel_name\": [\"Dali\"]\\n}'"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# with specific instructions, the second prompt can filter out the unnecessary information\n",
        "post_processing_result.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Vessel Removal to Take Place at Baltimore Port by May 10',\n",
              " 'summary': \"The Dali containership, which collided with the Francis Scott Key Bridge and has been lodged in the debris since March 26, is expected to be removed by May 10, according to officials involved in the clear-up efforts at Baltimore Port in the US. The removal is part of a larger operation that includes the recovery of two missing roadworkers and the reopening of the port's permanent channel by the end of May.\",\n",
              " 'impact': 'Yes',\n",
              " 'reasoning': 'The event directly impacts the supply chain network by disrupting the normal operations of the Port of Baltimore, which is a major transportation hub for goods and people. The collapse of the bridge has created a challenge in the removal of the vessel, and the authorities have set a target date of May 10 for its removal. This indicates that the event has the potential to cause significant disruption to the supply chain network, and therefore, the impact assessment is Yes.',\n",
              " 'vessel_name': ['Dali']}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval(post_processing_result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. LLAMA-3-8B-INSTRUCT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1716542807470
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "LLAMA3_8B_INSTRUCT_ENDPOINT = os.getenv(\"LLAMA3_8B_INSTRUCT_ENDPOINT\")\n",
        "LLAMA3_8B_INSTRUCT_API_KEY = os.getenv(\"LLAMA3_8B_INSTRUCT_API_KEY\")\n",
        "ENDPOINT_URL = LLAMA3_8B_INSTRUCT_ENDPOINT + \"/v1/chat/completions\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "gather": {
          "logged": 1716544371271
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/langchain-demo-env/lib/python3.11/site-packages/langchain_community/chat_models/azureml_endpoint.py:180: UserWarning: `LlamaChatContentFormatter` will be deprecated in the future. \n",
            "                Please use `CustomOpenAIChatContentFormatter` instead.  \n",
            "            \n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "chat_model = AzureMLChatOnlineEndpoint(\n",
        "    endpoint_url = ENDPOINT_URL,\n",
        "    endpoint_api_type=AzureMLEndpointApiType.serverless,\n",
        "    endpoint_api_key=LLAMA3_8B_INSTRUCT_API_KEY,\n",
        "    content_formatter=LlamaChatContentFormatter(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1716543685553
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "NEWS_PARSING_PROMPT = \"\"\"\n",
        "You are an expert extraction algorithm, specialized in news analysis.\n",
        "Your goal is to understand and parse out the news article content based on the user's instructions of the output schema.\n",
        "Only ouput the result into the schema without generating any other information outside the schema.\n",
        "{format_instructions}\n",
        "\n",
        "news article content:\n",
        "{news_content}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    template=NEWS_PARSING_PROMPT,\n",
        "    partial_variables = {\n",
        "        \"format_instructions\": format_instructions # passing in the formatting instructions created earlier in place of \"format_instructions\" placeholder\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "llm = chat_model\n",
        "full_chain = {\"news_content\": lambda x: x[\"news_content\"]} | prompt | llm\n",
        "result = full_chain.invoke({\"news_content\": sample_text}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'```\\n{\\n    \"title\": \"Bridge collapse at Baltimore Port: Vessel removal planned for 10 May\",\\n    \"summary\": \"The Dali containership will be removed from the collapsed Francis Scott Key Bridge at Baltimore Port by 10 May, according to officials. The removal will allow for the reopening of the Port\\'s permanent channel.\",\\n    \"impact\": \"Yes\",\\n    \"reasoning\": \"The vessel\\'s removal will directly impact the supply chain network by reopening the Port\\'s permanent channel, allowing ships to navigate in and out of the port.\",\\n    \"vessel_name\": [\"Dali\", \"Francis Scott Key Bridge\"]\\n}\\n```'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# we can see that the instruct model, despite having smaller size, is able to follow instruction better (although the quality of the output may not be very good)\n",
        "# however, there are still unnecessary characters before and after the JSON output\n",
        "result.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "NEWS_PARSING_PROMPT_MODIFIED = \"\"\"\n",
        "You are an expert extraction algorithm, specialized in news analysis.\n",
        "Your goal is to understand and parse out the news article content based on the user's instructions of the output schema.\n",
        "Only ouput the result into the schema without generating any other information outside the schema.\n",
        "Do not output extra characters before or after the JSON output, such as ```, so that the result can be parsed to a proper JSON later, while ensuring the JSON format is correct.\n",
        "{format_instructions}\n",
        "\n",
        "news article content:\n",
        "{news_content}\n",
        "\"\"\"\n",
        "\n",
        "prompt_modified = ChatPromptTemplate.from_template(\n",
        "    template=NEWS_PARSING_PROMPT_MODIFIED,\n",
        "    partial_variables = {\n",
        "        \"format_instructions\": format_instructions # passing in the formatting instructions created earlier in place of \"format_instructions\" placeholder\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "llm = chat_model\n",
        "full_chain_modified = {\"news_content\": lambda x: x[\"news_content\"]} | prompt_modified | llm\n",
        "result_modified = full_chain.invoke({\"news_content\": sample_text}) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\\n    \"title\": \"Baltimore Port Bridge Collapse: Vessel to be Removed by 10 May\",\\n    \"summary\": \"The Dali containership remains lodged in the debris of the Francis Scott Key Bridge at Baltimore Port in the US, which collapsed on March 26. Officials plan to remove the vessel by 10 May, but the clearance operation is complicated and poses safety risks. The port\\'s 50ft deep and 700ft wide channel is expected to reopen by the end of May.\",\\n    \"impact\": \"Yes\",\\n    \"reasoning\": \"The bridge collapse and subsequent vessel collision have the potential to disrupt the normal operations of the port, causing delays and impacting the movement of goods. The clearance operation is complicated and poses safety risks, which could further impact the port\\'s operations.\",\\n    \"vessel_name\": [\"Dali\", \"Francis Scott Key Bridge\"]\\n}'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# you can see that the model output is now in a cleaner JSON format\n",
        "result_modified.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'title': 'Baltimore Port Bridge Collapse: Vessel to be Removed by 10 May',\n",
              " 'summary': \"The Dali containership remains lodged in the debris of the Francis Scott Key Bridge at Baltimore Port in the US, which collapsed on March 26. Officials plan to remove the vessel by 10 May, but the clearance operation is complicated and poses safety risks. The port's 50ft deep and 700ft wide channel is expected to reopen by the end of May.\",\n",
              " 'impact': 'Yes',\n",
              " 'reasoning': \"The bridge collapse and subsequent vessel collision have the potential to disrupt the normal operations of the port, causing delays and impacting the movement of goods. The clearance operation is complicated and poses safety risks, which could further impact the port's operations.\",\n",
              " 'vessel_name': ['Dali', 'Francis Scott Key Bridge']}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval(result_modified.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "langchain-demo-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
